{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Final Project\n",
    "\n",
    "## Goals of the Project\n",
    "\n",
    "This project presents an attempt to quantify the predictability of the Bundesliga football games based on the information contained in game statistics staring from season 2006/07 onwards. In particular,\n",
    "\n",
    "* we use a deep neural network to examine the correlations between different features provided from the statistics of individual games.\n",
    "* we ask the question: \"to what accuracy canone determine the outcome and score of any Bundesliga game, given the games statistics from a number of previous seasons?\".\n",
    "* we compare the predictions of our model on the Bundesliga season 2016/17 up to the present matchday. We also make a prediction for the final Bundesliga table for season 2016/17.\n",
    "\n",
    "Studying the predictive power of footbal statistics on the outcome of games is an interesting and challenging problem of general importance both to sports, and the development of a proper intuition about the inner workings of machine learning techniques. The difficulty of the problem is enhanced by the occurence of outlier events in the data, for example a team performing better in a given game but nevertheless losing to their opponent 'misfortunately'. Such events can easily be recognised as noise or bias by the neural network leading to an increase in both the in-sample and out-of-sample errors. \n",
    "\n",
    "\n",
    "## Data and Features\n",
    "\n",
    "We use a manually developed football games dataset, collected from data provided at ... and kicker.de . We then modified those sets introducing some new features as combinations of the existing ones, while dropping others.\n",
    "\n",
    "Each data point is represented as a vector containing the following features: (list names of all features below)\n",
    "\n",
    "Last, each feature from the data was normalised to unity with respect to all games in the data set.\n",
    "\n",
    "To acquire a better understanding of the data used to train the deep neural net, below we show histrograms of selected interesting features, that are often used intuitively to estimate the outcome of a given fixture:\n",
    "\n",
    "1. Shots (again, two hists: H and A)\n",
    "\n",
    "2. Shots on target (let's plot the two hists (H and A) on top of each other with some transparency)\n",
    "\n",
    "3. Attendance\n",
    "\n",
    "4. 5, 6....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model: Deep Neural Network\n",
    "\n",
    "Our model is a deep relu neural net, consisting of an input layer, four hidden layers and an output layer:\n",
    "* input layer: ?? features\n",
    "* hidden layer 1: 50 neurons\n",
    "* hidden layer 2: 500 neurons\n",
    "* hidden layer 3: 500 neurons\n",
    "* hidden layer 4: 50 neurons\n",
    "* output layer with softmax activation: ?? features\n",
    "\n",
    "We train with minibatches using the 'adam' optimiser, and minimise the categorical entropy function. Additionally, we apply 'dropout' regularisation after hidden layer 4. \n",
    "\n",
    "here comes the code...\n",
    "\n",
    "## Results\n",
    "\n",
    "show some plots here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
