{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feats_import = pd.read_csv('All_Data_2006_2016.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAECCAYAAAD6oXArAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13853a090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "feats_import.loc[:, 'FTGD'].plot(kind='hist', bins=16)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test dropping out certain features, outdated. \n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# performance_list = []\n",
    "# for i in range(1,21,2):\n",
    "#     if i<3:\n",
    "#         continue\n",
    "#     np.random.seed(7)\n",
    "#     X_vary = np.delete(X,(i,i+1),1)\n",
    "    \n",
    "#     def baseline_model():\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(40, input_dim=len(X_vary[0]), init='uniform', activation='relu')) \n",
    "#         # model.add(Dropout(0.3))\n",
    "#         model.add(Dense(20, activation='sigmoid'))\n",
    "#         model.add(Dense(80, activation='relu'))\n",
    "#         model.add(Dense(2*cutoff_GD+1, activation='sigmoid'))\n",
    "\n",
    "#         model.compile(optimizer='adam',\n",
    "#                       loss='categorical_crossentropy',\n",
    "#                       metrics=['accuracy'])\n",
    "#         return model\n",
    "#     estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=20, batch_size=20, verbose=0)\n",
    "    \n",
    "#     results = cross_val_score(estimator, X_vary, y, cv=kfold)\n",
    "#     print feats.columns[i:i+2]\n",
    "#     print [i, 100*results.mean(), 100*results.std()]\n",
    "#     performance_list += [i, 100*results.mean(), 100*results.std()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outdated\n",
    "\n",
    "# np.random.seed(7)\n",
    "# X_vary = X\n",
    "\n",
    "# def baseline_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(40, input_dim=len(X_vary[0]), init='lecun_uniform', activation='relu')) \n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "#     model.add(Dense(2*cutoff_GD+1, activation='sigmoid'))\n",
    "#     model.add(Activation('softmax'))\n",
    "\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "# estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=100, batch_size=10, verbose=0)\n",
    "\n",
    "# results = cross_val_score(estimator, X_vary, y)\n",
    "# print 100*results.mean(), 100*results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FTHG</th>\n",
       "      <th>HTHG</th>\n",
       "      <th>HS</th>\n",
       "      <th>AS</th>\n",
       "      <th>HST</th>\n",
       "      <th>AST</th>\n",
       "      <th>HF</th>\n",
       "      <th>AF</th>\n",
       "      <th>HC</th>\n",
       "      <th>AC</th>\n",
       "      <th>HY</th>\n",
       "      <th>AY</th>\n",
       "      <th>HR</th>\n",
       "      <th>AR</th>\n",
       "      <th>HGA</th>\n",
       "      <th>AGA</th>\n",
       "      <th>HTGD</th>\n",
       "      <th>Odds</th>\n",
       "      <th>HP3</th>\n",
       "      <th>AP3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.427273</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.760007</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.775832</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FTHG  HTHG        HS        AS       HST       AST        HF        AF  \\\n",
       "0  0.000000   0.0  0.277778  0.305556  0.222222  0.242424  0.342857  0.685714   \n",
       "1  0.333333   0.4  0.555556  0.222222  0.305556  0.151515  0.628571  0.228571   \n",
       "2  0.222222   0.0  0.361111  0.305556  0.222222  0.212121  0.314286  0.685714   \n",
       "3  0.222222   0.2  0.333333  0.444444  0.250000  0.212121  0.400000  0.571429   \n",
       "4  0.111111   0.2  0.666667  0.222222  0.361111  0.151515  0.485714  0.542857   \n",
       "\n",
       "     HC    AC        HY        AY    HR    AR       HGA       AGA  HTGD  \\\n",
       "0  0.25  0.30  0.090909  0.181818  0.00  0.00  0.309091  0.727273  0.00   \n",
       "1  0.25  0.05  0.272727  0.000000  0.25  0.25  0.654545  0.300000  1.00   \n",
       "2  0.15  0.25  0.000000  0.363636  0.00  0.25  0.545455  0.427273  0.50   \n",
       "3  0.20  0.20  0.090909  0.181818  0.00  0.00  0.654545  0.381818  0.75   \n",
       "4  0.35  0.25  0.090909  0.181818  0.00  0.00  0.581818  0.363636  0.75   \n",
       "\n",
       "       Odds  HP3  AP3  \n",
       "0  0.656250  0.5  0.5  \n",
       "1  0.819444  0.5  0.5  \n",
       "2  0.760007  0.5  0.5  \n",
       "3  0.775832  0.5  0.5  \n",
       "4  0.819444  0.5  0.5  "
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal differential Cut-off.\n",
    "cutoff_GD = 2\n",
    "\n",
    "feats = feats_import.drop(['Season', 'Gameday', 'TID_H', 'TID_A'], axis=1).drop('FTGD', axis=1)\n",
    "feats.loc[:, 'HTGD'] = feats_import.loc[:, 'HTGD'].apply(min, args=(cutoff_GD,)).apply(max, args=(-cutoff_GD,)) + cutoff_GD\n",
    "\n",
    "label = feats_import.loc[:, 'FTGD'].apply(min, args=(cutoff_GD,)).apply(max, args=(-cutoff_GD,)) + cutoff_GD\n",
    "\n",
    "GD_min = -cutoff_GD\n",
    "GD_max = +cutoff_GD\n",
    "GD_spread = GD_max-GD_min + 1\n",
    "\n",
    "\n",
    "# Different norms because I played around with different columns\n",
    "norm = [9, 5, 36, 36, 36, 33, 35, 35, 20, 20, 11, 11, 4, 4, 1, 1, GD_spread-1, 1, 1, 1]\n",
    "#norm = [9, 5, 36, 36, 36, 33, 35, 35, 20, 20, 1, 1, GD_spread-1, 1, 1, 1]\n",
    "#norm = [9, 5, 36, 36, 36, 33, 35, 35, 20, 20, 11, 11, 4, 4, GD_spread-1, 1, 1, 1]\n",
    "\n",
    "feats = feats/norm\n",
    "\n",
    "\n",
    "ID = np.eye(GD_spread)\n",
    "\n",
    "X = feats.iloc[:3000].as_matrix()\n",
    "y_pre = map(int, label.iloc[:3000].as_matrix())\n",
    "\n",
    "y = np.array([ID[i] for i in y_pre])\n",
    "\n",
    "feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign an expected score with probabilities\n",
    "def exp_score(x):\n",
    "    multiplier = np.array(range(-cutoff_GD, cutoff_GD+1))\n",
    "    return np.sum(np.array(x) * multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.720333334506\n",
      "2368/3000 [======================>.......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Run Keras\n",
    "np.random.seed(7)\n",
    "\n",
    "# I tested these parameters a little bit; they seem to work nicely for know.\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=len(X[0]), init='lecun_uniform', activation='relu')) \n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2*cutoff_GD+1, activation='relu'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=100, batch_size=10, verbose=False)\n",
    "print model.evaluate(X, y, batch_size=10, verbose=False)[1]\n",
    "\n",
    "predictions = model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npIYZApLUMENAkhpmCEhSwwwBSWrY/wLEXPUcX0IKCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125a63110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Matrix Plot to visualize predictions vs. outcome\n",
    "\n",
    "# x-axis: Actual outcome\n",
    "# y-axis: Expected score (EXP)\n",
    "plt_x = np.array(performance_df.loc[:, \"FTGD\"])\n",
    "plt_y = np.array(performance_df.loc[:, \"EXP\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.hist2d(plt_x, plt_y, bins=2*cutoff_GD+1, range=np.array([(-cutoff_GD,cutoff_GD), (-cutoff_GD,cutoff_GD)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success identifying H, D, A is 85.9 percent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>EXP</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTGD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>812</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>141</td>\n",
       "      <td>500</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "EXP   -1.0   0.0   1.0\n",
       "FTGD                  \n",
       "-1.0   812    86     4\n",
       " 0.0   141   500   101\n",
       " 1.0     3    88  1265"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Tabulation for Away-Win (-1), Draw (0) or Home-win (1)\n",
    "performance_df = pd.concat([\n",
    "        feats_import.iloc[:3000,-5].apply(min, args=(cutoff_GD,)).apply(max, args=(-cutoff_GD,)),\n",
    "        pd.Series(data=map(np.round, map(exp_score, predictions)), name='EXP', index=feats_import.iloc[:3000].index)], axis=1)\n",
    "success_res_df = pd.crosstab(performance_df.loc[:, \"FTGD\"].apply(np.sign), performance_df.loc[:, \"EXP\"].apply(np.sign))\n",
    "print \"Success identifying H, D, A is \" + str(round(100* np.trace(success_res_df)/3000.,2)) + \" percent\"\n",
    "success_res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success identifying GD is 69.3 percent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>EXP</th>\n",
       "      <th>-2.0</th>\n",
       "      <th>-1.0</th>\n",
       "      <th>-0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTGD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2.0</th>\n",
       "      <td>148</td>\n",
       "      <td>292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>7</td>\n",
       "      <td>365</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>500</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>382</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "EXP   -2.0  -1.0  -0.0   1.0   2.0\n",
       "FTGD                              \n",
       "-2.0   148   292     3     0     0\n",
       "-1.0     7   365    83     4     0\n",
       " 0.0     0   141   500    97     4\n",
       " 1.0     0     3    88   382   124\n",
       " 2.0     0     0     0    75   684"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-Tabulation for exact goal differential\n",
    "\n",
    "success_df = pd.crosstab(performance_df.loc[:, \"FTGD\"], performance_df.loc[:, \"EXP\"])#, margins=True)\n",
    "exp_min = int(performance_df.loc[:, \"EXP\"].min())\n",
    "exp_max = int(performance_df.loc[:, \"EXP\"].max())\n",
    "print \"Success identifying GD is \" + str(round(np.sum([100*success_df.ix[i,i] for i in range(exp_min,exp_max+1)])/3000.,2)) + \" percent\"\n",
    "success_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.03, 0.9, 0.07, 0.0, 0.0]\n",
      "0.03484211881\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Check one example.\n",
    "i=1123\n",
    "print [round(predictions[i][j], 2) for j in range(len(predictions[i]))]\n",
    "print exp_score(predictions[i])\n",
    "print label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
