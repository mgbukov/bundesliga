{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def moveSibling(tag, number):\n",
    "\ti = 1\n",
    "\twhile i <= number:\n",
    "\t\ttag = tag.nextSibling\n",
    "\t\ti += 1\n",
    "\treturn tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up dataframes to store the downloaded data\n",
    "# Table for game information\n",
    "matches_columns = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA',\n",
    "                   'GoalsH', 'ShotsH', 'PassesH', 'PassQuH', 'PossesH', \n",
    "                   'ChallH', 'FoulsH', 'OffsideH', 'CornersH', 'ChancesH',\n",
    "                   'GoalsA', 'ShotsA', 'PassesA', 'PassQuA', 'PossesA', \n",
    "                   'ChallA', 'FoulsA', 'OffsideA', 'CornersA', 'ChancesA',\n",
    "                   'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', \n",
    "                   'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = pd.DataFrame(data=None, columns=matches_columns)\n",
    "\n",
    "# Table for goal information, can be linked through \"Link\" with game info\n",
    "goals_columns = ['Season', 'Gameday', 'Link', 'Score', 'Minute', 'Player']\n",
    "goals_df = pd.DataFrame(data=None, columns=goals_columns)\n",
    "\n",
    "# Table for Starting 11\n",
    "lineup_columns = [\"Link\"] + \\\n",
    "                    [\"Home_Player\"+(\"0\"+str(i))[-2:] for i in range(1,12)] + \\\n",
    "                    [\"Away_Player\"+(\"0\"+str(i))[-2:] for i in range(1,12)]\n",
    "lineup_df = pd.DataFrame(data=None, columns=lineup_columns)\n",
    "\n",
    "for year in range(2014,2015):\n",
    "    for gameday in range(1,35):\n",
    "        # Write URL to access kicker gameday website\n",
    "        url = \"http://www.kicker.de/news/fussball/bundesliga/spieltag/1-bundesliga/\"\n",
    "        url = url + str(year) + \"-\" + str(year+1)[2:4] + \"/\"\n",
    "        url = url + str(gameday) + \"/0/spieltag.html\"\n",
    "        \n",
    "        # Avoid overloading with requests and pause if necessary\n",
    "        file = ''\n",
    "        while file =='':\n",
    "            try:\n",
    "                file = requests.get(url)\n",
    "            except:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        # Download and parse page\n",
    "        main = BeautifulSoup(file.content, 'html.parser')\n",
    "        \n",
    "        # Games are listed in a table        \n",
    "        main_matches = main.find('table', \n",
    "                                 {'class': 'tStat tab1-bundesliga', \n",
    "                                  'summary': 'Begegnungen'})\n",
    "\n",
    "        # Find all links of class \"link.\" These refer to the game analyses.\n",
    "        # In the analyses, we find (most of) the game stats\n",
    "        for game in main_matches.find_all('a', {'class' : 'link'}):\n",
    "            game_url = game.get('href')\n",
    "            encoded_in_url = game_url.split(\"/\")\n",
    "\n",
    "            # Game ID is the article ID for kicker, as unique ID for game\n",
    "            game_id = encoded_in_url[8][0:7]\n",
    "\n",
    "            # Game URL encodes the teams which are playing\n",
    "            teams = encoded_in_url[9][13:-5].split(\"_\")\n",
    "            teams = [teams[i][teams[i].rfind('-')+1:] for i in range(2)]\n",
    "            \n",
    "            # There is a different webpage for historic info on the teams.\n",
    "            # We only have to swap out part of the url\n",
    "            game_url = \"http://www.kicker.de\" + game_url\n",
    "            hist_file = requests.get(game_url.replace(\"spielanalyse\", \n",
    "                                                      \"direktvergleichliga\"))\n",
    "            hist_soup = BeautifulSoup(hist_file.content, 'html.parser')\n",
    "\n",
    "            # Can't use \"Direktvergleich\" because it includes all matches \n",
    "            # up to date of download and not to date of that game.\n",
    "            # hist = hist_soup.find('table', {'class': 'tStat', \n",
    "            #                                 'summary': 'Direktvergleich'})\n",
    "            # record = map(int, map(str.strip, str(hist.find('td', {'class': \"alignright spielinfo_wert\"}).text).split('/')))\n",
    "            \n",
    "            hist = hist_soup.find('table', \n",
    "                                  {'class': 'tStat tab1-bundesliga', \n",
    "                                   'summary': 'Begegnungen'})\n",
    "            \n",
    "            # Empty list for the results of previous matchups of the teams\n",
    "            results = []\n",
    "            try:\n",
    "                for g in hist.find_all('td', {'class': 'alignleft nowrap'}):\n",
    "                    link = g.next_sibling.next_sibling.find('a').get('href')\n",
    "                    encoded_in_url = link.split(\"/\")\n",
    "\n",
    "                    # Date of matches in list of all matchups of these teams\n",
    "                    res_year = int(encoded_in_url[6][0:4])\n",
    "                    res_gday = int(encoded_in_url[7])\n",
    "                    \n",
    "                    # Only include games prior to this one!\n",
    "                    if res_year < int(year) or (res_year == year and res_gday < gameday):\n",
    "                        result = g.text\n",
    "                        result = map(int, result[0:result.find('(')].strip().split(\":\"))\n",
    "                        result = result[0]-result[1]\n",
    "                        # If the historical game was Team B - Team A\n",
    "                        # instead of Team A - Team B today, then flip result\n",
    "                        if link.find(teams[0] + \".html\") != -1:\n",
    "                            result = -result\n",
    "                        results.append(result)\n",
    "                        \n",
    "                    # Only interested in the last 6\n",
    "                    if len(results) >= 6:\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # If there were less than 6 games, fill up with NaN\n",
    "            results += [np.nan] * (6 - len(results))\n",
    "\n",
    "\n",
    "            # Open website with statistics from the game and parse.\n",
    "            game_file = requests.get(game_url)\n",
    "            game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "            \n",
    "            # Error handling if website is malformatted or other.\n",
    "            error_count = 0\n",
    "            while game_soup.find('table', {'class': 'tStat', \n",
    "                                           'summary': 'Vereinsliste'}) == None:\n",
    "                game_file = requests.get(game_url)\n",
    "                game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "                error_count += 1\n",
    "                \n",
    "                if error_count == 3:\n",
    "                    print \"Error on game \" + str(game_id)\n",
    "                    break\n",
    "\n",
    "            # Download lineups\n",
    "            # Error handling again; if problems with lineup, write NaN and continue\n",
    "            if error_count < 3:\n",
    "                both_teams = game_soup.find('table', {'class': 'tStat', \n",
    "                                                      'summary': 'Vereinsliste'})\n",
    "                lineup = []\n",
    "                for one_team in both_teams.find_all('div', \n",
    "                                                    {'class': 'aufstellung_team'}):\n",
    "                    for player in one_team.find_all('a', \n",
    "                                                    {'class': 'link_noicon'}):\n",
    "                        lineup.append(player.get('href').split(\"/\")[8])\n",
    "                        # Make sure to only download the first 11, \n",
    "                        # the Starting 11, for each team\n",
    "                        if np.mod(len(lineup), 11) == 0:\n",
    "                            break\n",
    "                lineup = [int(game_id)] + map(int, lineup)\n",
    "            else:\n",
    "                lineup = [int(game_id)] + [np.nan for _ in range(22)]\n",
    "                \n",
    "            lineup_df = lineup_df.append(pd.DataFrame(data=[lineup], \n",
    "                                                      columns=lineup_columns),\n",
    "                                         ignore_index=True)\n",
    "\n",
    "            # Download main stats and save in a list.\n",
    "            stats = game_soup.find('table', {'class': 'tStat tStatKarten', \n",
    "                                             'summary': 'Berufungen'})\n",
    "            team1_stats = []\n",
    "            team2_stats = []\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignleft first\"}):\n",
    "                    if stat.text != ('angekommene Pässe').decode('utf-8') \\\n",
    "                        and stat.text != ('Ecken').decode('utf-8') \\\n",
    "                        and stat.text != ('Fehlpässe').decode('utf-8') \\\n",
    "                        and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                            team1_stats.append(stat.next_sibling.next_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team1_stats = [np.NaN for _ in range(8)]\n",
    "                team1_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainH'}).text)\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignright last\"}):\n",
    "                    if stat.text != ('angekommene Pässe').decode('utf-8') \\\n",
    "                        and stat.text != ('Ecken').decode('utf-8') \\\n",
    "                        and stat.text != ('Fehlpässe').decode('utf-8') \\\n",
    "                        and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                            team2_stats.append(stat.previous_sibling.previous_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team2_stats = [np.NaN for _ in range(8)]\n",
    "                team2_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainA'}).text)\n",
    "\n",
    "                \n",
    "            # For whatever reason, some stats are stored in a different table. \n",
    "            # Grab this and add to list.     \n",
    "            for value in game_soup.findAll('div', \n",
    "                                           id=['ctl00_PlaceHolderHalf_ctl03_kickerDaten', \n",
    "                                               'ctl00_PlaceHolderHalf_ctl04_kickerDaten']):\n",
    "                corners = value.find('div', {'class': 'ecken'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(corners[0]))\n",
    "                team2_stats.append(int(corners[1]))\n",
    "                chances = value.find('div', {'class': 'chancen'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(chances[0]))\n",
    "                team2_stats.append(int(chances[1]))\n",
    "\n",
    "            # Download number of people in attendance\n",
    "            for value in game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_zuschauer', \n",
    "                                                      'ctl00_PlaceHolderHalf_ctl04_zuschauer']):\n",
    "                attendance = value.find('div', {'class': 'wert'}).text\n",
    "                if attendance.find('(') > 0:\n",
    "                    # \"Error\" handling if stadion is sold out\n",
    "                    attendance = attendance[0:attendance.find('(')]\n",
    "                attendance = int(attendance.strip())\n",
    "\n",
    "\n",
    "            # Write stats for game into dataframe\n",
    "            feed = [year] + [gameday] + [game_id] + teams + \\\n",
    "                    team1_stats + team2_stats + [attendance] + results\n",
    "            feed += [np.nan] * (len(matches_columns) - len(feed))\n",
    "            matches_df = matches_df.append(pd.DataFrame(data=[feed],\n",
    "                                                        columns=matches_columns), \n",
    "                                           ignore_index=True)\n",
    "\n",
    "            # Find the scorers in the game and download the info\n",
    "            goal_list = []\n",
    "            goals_cards = game_soup.find('table', {'class': 'tStat', 'summary': 'Tore & Karten'})\n",
    "            goals = goals_cards.find('div', {'class': 'tore_karten'})\n",
    "            goals = goals.find_all('div', {'class': 'kompletteZeile'})\n",
    "\n",
    "            for goal in goals:\n",
    "                goal_text = goal.text\n",
    "                goal_link = goal.find('a')\n",
    "                player = goal_link.get('href').split(\"/\")[8]\n",
    "\n",
    "                pos = goal_text.find(\":\")\n",
    "                score = int(goal_text[pos-1:pos])-int(goal_text[pos+1:pos+2])\n",
    "                pos = goal_text.find(\".,\")\n",
    "                # \"Error\" handling for when a goal is scored in overtime. Treat as 45' and 90'.\n",
    "                if pos < 0:\n",
    "                    pos = goal_text.find(\". +\") \n",
    "                minute = goal_text[0:pos]\n",
    "                pos = minute.find(\"(\")\n",
    "                minute = int(minute[pos+1:])\n",
    "\n",
    "                goal_list.append(map(int,[year, gameday, game_id, score, minute, player]))\n",
    "\n",
    "            # If goals fell, write info into goal dataframe.    \n",
    "            if len(goal_list) > 0:\n",
    "                goals_df = goals_df.append(pd.DataFrame(data=goal_list, \n",
    "                                                        columns=goals_columns), \n",
    "                                           ignore_index=True)\n",
    "                \n",
    "for col in matches_df.columns:\n",
    "    try:\n",
    "        matches_df.loc[:, col] = matches_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "matches_df.loc[:, \"GD\"] = matches_df.loc[:, \"GoalsH\"] - matches_df.loc[:, \"GoalsA\"]\n",
    "\n",
    "matches_columns_ro = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA', 'GD',\n",
    "                      'GoalsH', 'GoalsA', 'ShotsH', 'ShotsA', 'PassesH', \n",
    "                      'PassesA', 'PassQuH', 'PassQuA', 'PossesH', 'PossesA', \n",
    "                      'ChallH', 'ChallA', 'FoulsH', 'FoulsA', 'OffsideH', 'OffsideA', \n",
    "                      'CornersH', 'CornersA', 'ChancesH', 'ChancesA', 'Attendance', \n",
    "                      'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = matches_df.loc[:, matches_columns_ro]\n",
    "\n",
    "for col in goals_df.columns:\n",
    "    try:\n",
    "        goals_df.loc[:, col] = goals_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for col in lineup_df.columns:\n",
    "    try:\n",
    "        lineup_df.loc[:, col] = lineup_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "matches_df.to_csv(\"Matches_2014.csv\")\n",
    "goals_df.to_csv(\"Goals_2014.csv\")\n",
    "lineup_df.to_csv(\"Lineup_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to download table/standings at any given game day.\n",
    "# Easier though to calculate on the fly.\n",
    "\n",
    "# main_standings = main.find('table', {'class': 'tStat', 'summary': 'Tabelle'})\n",
    "# main_standings = main_standings.find_all('tr', {'class': ['fest ', 'fest alt']})\n",
    "\n",
    "# for team in main_standings:\n",
    "#     # Start with the link for the team as our anchor\n",
    "#     current_tag = team\n",
    "#     # Split up the URL that encodes the team name & ID, and then separate the team ID from the teamname\n",
    "#     team_id = int(current_tag.get('href').split('/')[7].split('-')[-1])\n",
    "#     # Move to the win-draw-loss record\n",
    "#     current_tag = current_tag.parent.parent.parent\n",
    "#     current_tag = moveSibling(current_tag, 8)\n",
    "#     rec_w = int(current_tag.text)\n",
    "#     current_tag = moveSibling(current_tag, 2)\n",
    "#     rec_d = int(current_tag.text)\n",
    "#     current_tag = moveSibling(current_tag, 2)\n",
    "#     rec_l = int(current_tag.text)\n",
    "#     current_tag = moveSibling(current_tag, 4)\n",
    "#     goals = map(int, current_tag.text.split(\":\"))\n",
    "\n",
    "#     print [team_id] + [rec_w] + [rec_d] + [rec_l] + goals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
