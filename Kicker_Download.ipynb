{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def moveSibling(tag, number):\n",
    "\ti = 1\n",
    "\twhile i <= number:\n",
    "\t\ttag = tag.nextSibling\n",
    "\t\ti += 1\n",
    "\treturn tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up dataframes to store the downloaded data\n",
    "# Table for game information\n",
    "matches_columns = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA',\n",
    "           'GoalsH', 'ShotsH', 'PassesH', 'PassQuH', 'PossesH', 'ChallH', 'FoulsH', 'OffsideH', 'CornersH', 'ChancesH',\n",
    "           'GoalsA', 'ShotsA', 'PassesA', 'PassQuA', 'PossesA', 'ChallA', 'FoulsA', 'OffsideA', 'CornersA', 'ChancesA',\n",
    "           'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = pd.DataFrame(data=None, columns=matches_columns)\n",
    "\n",
    "# Table for goal information, can be linked through \"Link\" with game info\n",
    "goals_columns = ['Season', 'Gameday', 'Link', 'Score', 'Minute', 'Player']\n",
    "goals_df = pd.DataFrame(data=None, columns=goals_columns)\n",
    "\n",
    "# Table for Starting 11\n",
    "lineup_columns = [\"Link\"] + [\"Home_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)] + [\"Away_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)]\n",
    "lineup_df = pd.DataFrame(data=None, columns=lineup_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lineup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for a in main_table.find_all('a', {'class': 'link verinsLinkBild', 'href': re.compile('/news/fussball/bundesliga/vereine/1-bundesliga/2004-05')}):\n",
    "#    print a.parent.parent.parent.previousSibling.previousSibling.previousSibling.previousSibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(2004,2005):\n",
    "    for gameday in range(1,35):\n",
    "        # Write URL to access kicker gameday website\n",
    "        url = \"http://www.kicker.de/news/fussball/bundesliga/spieltag/1-bundesliga/\"\n",
    "        url = url + str(year) + \"-\" + str(year+1)[2:4] + \"/\"\n",
    "        url = url + str(gameday) + \"/0/spieltag.html\"\n",
    "        \n",
    "        file = ''\n",
    "        while file =='':\n",
    "            try:\n",
    "                file = requests.get(url)\n",
    "            except:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        # Download and parse page\n",
    "        main = BeautifulSoup(file.content, 'html.parser')\n",
    "\n",
    "        main_table = main.find('table', {'class': 'tStat', 'summary': 'Tabelle'})\n",
    "        \n",
    "        main_matches = main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'})\n",
    "\n",
    "        # Find all links of class \"link.\" These refer to the game analyses.\n",
    "        for game in main_matches.find_all('a', {'class' : 'link'}):\n",
    "            game_url = game.get('href')\n",
    "            encoded_in_url = game_url.split(\"/\")\n",
    "\n",
    "            # Game ID is the article ID for kicker, as unique ID for game\n",
    "            game_id = encoded_in_url[8][0:7]\n",
    "\n",
    "            # Game URL encodes the teams which are playing\n",
    "            teams = encoded_in_url[9][13:-5].split(\"_\")\n",
    "            teams = [teams[i][teams[i].rfind('-')+1:] for i in range(2)]\n",
    "\n",
    "            game_url = \"http://www.kicker.de\" + game_url\n",
    "            hist_file = requests.get(game_url.replace(\"spielanalyse\", \"direktvergleichliga\"))\n",
    "            hist_soup = BeautifulSoup(hist_file.content, 'html.parser')\n",
    "\n",
    "            # Can't use \"Direktvergleich\" because it includes all matches up to date of download and not to date of that game.\n",
    "            # hist = hist_soup.find('table', {'class': 'tStat', 'summary': 'Direktvergleich'})\n",
    "            # record = map(int, map(str.strip, str(hist.find('td', {'class': \"alignright spielinfo_wert\"}).text).split('/')))\n",
    "            hist = hist_soup.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'})\n",
    "            results = []\n",
    "            try:\n",
    "                for g in hist.find_all('td', {'class': 'alignleft nowrap'}):\n",
    "                    link = g.next_sibling.next_sibling.find('a').get('href')\n",
    "                    encoded_in_url = link.split(\"/\")\n",
    "\n",
    "                    res_year = int(encoded_in_url[6][0:4])\n",
    "                    res_gday = int(encoded_in_url[7])\n",
    "\n",
    "                    if res_year < int(year) or (res_year == year and res_gday < gameday):\n",
    "                        result = g.text\n",
    "                        result = map(int, result[0:result.find('(')].strip().split(\":\"))\n",
    "                        result = result[0]-result[1]\n",
    "                        # If the historical game was Team B - Team A instead of Team A - Team B today, then flip result\n",
    "                        if link.find(teams[0] + \".html\") != -1:\n",
    "                            result = -result\n",
    "                        results.append(result)\n",
    "                    if len(results) >= 6:\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            results += [np.nan] * (6 - len(results))\n",
    "\n",
    "\n",
    "            # Open website with statistics from the game and parse.\n",
    "            game_file = requests.get(game_url)\n",
    "            game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "            \n",
    "            error_count = 0\n",
    "            while game_soup.find('table', {'class': 'tStat', 'summary': 'Vereinsliste'}) == None:\n",
    "                game_file = requests.get(game_url)\n",
    "                game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "                error_count += 1\n",
    "                \n",
    "                if error_count == 3:\n",
    "                    print \"Error on game \" + str(game_id)\n",
    "                    break\n",
    "\n",
    "            # Download lineups\n",
    "            if error_count < 3:\n",
    "                both_teams = game_soup.find('table', {'class': 'tStat', 'summary': 'Vereinsliste'})\n",
    "                lineup = []\n",
    "                for one_team in both_teams.find_all('div', {'class': 'aufstellung_team'}):\n",
    "                    for player in one_team.find_all('a', {'class': 'link_noicon'}):\n",
    "                        lineup.append(player.get('href').split(\"/\")[8])\n",
    "                        # Make sure to only download the first 11, the Starting 11, for each team\n",
    "                        if np.mod(len(lineup), 11) == 0:\n",
    "                            break\n",
    "                lineup = [int(game_id)] + map(int, lineup)\n",
    "            else:\n",
    "                lineup = [int(game_id)] + [np.nan for _ in range(22)]\n",
    "                \n",
    "            lineup_df = lineup_df.append(pd.DataFrame(data=[lineup], columns=lineup_columns), ignore_index=True)\n",
    "\n",
    "            # Download main stats and save in a list.\n",
    "            stats = game_soup.find('table', {'class': 'tStat tStatKarten', 'summary': 'Berufungen'})\n",
    "            team1_stats = []\n",
    "            team2_stats = []\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignleft first\"}):\n",
    "                    if stat.text != ('angekommene P채sse').decode('utf-8') and stat.text != ('Ecken').decode('utf-8') and stat.text != ('Fehlp채sse').decode('utf-8') and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                        team1_stats.append(stat.next_sibling.next_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team1_stats = [np.NaN for _ in range(8)]\n",
    "                team1_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainH'}).text)\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignright last\"}):\n",
    "                    if stat.text != ('angekommene P채sse').decode('utf-8') and stat.text != ('Ecken').decode('utf-8') and stat.text != ('Fehlp채sse').decode('utf-8') and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                        team2_stats.append(stat.previous_sibling.previous_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team2_stats = [np.NaN for _ in range(8)]\n",
    "                team2_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainA'}).text)\n",
    "\n",
    "            # For whatever reason, number of chances is stored in a different table. Grab this and add to list.     \n",
    "            for value in game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_kickerDaten', 'ctl00_PlaceHolderHalf_ctl04_kickerDaten']):\n",
    "                corners = value.find('div', {'class': 'ecken'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(corners[0]))\n",
    "                team2_stats.append(int(corners[1]))\n",
    "                chances = value.find('div', {'class': 'chancen'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(chances[0]))\n",
    "                team2_stats.append(int(chances[1]))\n",
    "\n",
    "            # Download number of people in attendance\n",
    "            for value in game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_zuschauer', 'ctl00_PlaceHolderHalf_ctl04_zuschauer']):\n",
    "                attendance = value.find('div', {'class': 'wert'}).text\n",
    "                if attendance.find('(') > 0:\n",
    "                    # \"Error\" handling if stadion is sold out\n",
    "                    attendance = attendance[0:attendance.find('(')]\n",
    "                attendance = int(attendance.strip())\n",
    "\n",
    "\n",
    "            # Write stats for game into dataframe\n",
    "\n",
    "            feed = [year] + [gameday] + [game_id] + teams + team1_stats + team2_stats + [attendance] + results\n",
    "            feed += [np.nan] * (len(matches_columns) - len(feed))\n",
    "            matches_df = matches_df.append(pd.DataFrame(data=[feed], columns=matches_columns), ignore_index=True)\n",
    "\n",
    "            # Find the scorers in the game and download the info\n",
    "            goal_list = []\n",
    "            goals_cards = game_soup.find('table', {'class': 'tStat', 'summary': 'Tore & Karten'})\n",
    "            goals = goals_cards.find('div', {'class': 'tore_karten'}).find_all('div', {'class': 'kompletteZeile'})\n",
    "\n",
    "            for goal in goals:\n",
    "                goal_text = goal.text\n",
    "                goal_link = goal.find('a')\n",
    "                player = goal_link.get('href').split(\"/\")[8]\n",
    "\n",
    "                pos = goal_text.find(\":\")\n",
    "                score = int(goal_text[pos-1:pos])-int(goal_text[pos+1:pos+2])\n",
    "                pos = goal_text.find(\".,\")\n",
    "                # \"Error\" handling for when a goal is scored in overtime. Treat as 45' and 90'.\n",
    "                if pos < 0:\n",
    "                    pos = goal_text.find(\". +\") \n",
    "                minute = goal_text[0:pos]\n",
    "                pos = minute.find(\"(\")\n",
    "                minute = int(minute[pos+1:])\n",
    "\n",
    "                goal_list.append(map(int,[year, gameday, game_id, score, minute, player]))\n",
    "\n",
    "            # If goals fell, write info into goal dataframe.    \n",
    "            if len(goal_list) > 0:\n",
    "                goals_df = goals_df.append(pd.DataFrame(data=goal_list, columns=goals_columns), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in matches_df.columns:\n",
    "    try:\n",
    "        matches_df.loc[:, col] = matches_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "matches_df.loc[:, \"GD\"] = matches_df.loc[:, \"GoalsH\"] - matches_df.loc[:, \"GoalsA\"]\n",
    "\n",
    "matches_columns_ro = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA', 'GD',\n",
    "                      'GoalsH', 'GoalsA', 'ShotsH', 'ShotsA', 'PassesH', 'PassesA', 'PassQuH', 'PassQuA', 'PossesH', 'PossesA', \n",
    "                      'ChallH', 'ChallA', 'FoulsH', 'FoulsA', 'OffsideH', 'OffsideA', 'CornersH', 'CornersA', 'ChancesH', 'ChancesA',\n",
    "                      'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = matches_df.loc[:, matches_columns_ro]\n",
    "\n",
    "\n",
    "matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in goals_df.columns:\n",
    "    goals_df.loc[:, col] = goals_df.loc[:, col].astype(int)\n",
    "goals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in lineup_df.columns:\n",
    "    lineup_df.loc[:, col] = lineup_df.loc[:, col].astype(int)\n",
    "lineup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matches_df.to_csv(\"Matches_2004.csv\")\n",
    "goals_df.to_csv(\"Goals_2004.csv\")\n",
    "lineup_df.to_csv(\"Lineup_2004.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up dataframes to store the downloaded data\n",
    "# Table for game information\n",
    "matches_columns = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA',\n",
    "           'GoalsH', 'ShotsH', 'PassesH', 'PassQuH', 'PossesH', 'ChallH', 'FoulsH', 'OffsideH', 'CornersH', 'ChancesH',\n",
    "           'GoalsA', 'ShotsA', 'PassesA', 'PassQuA', 'PossesA', 'ChallA', 'FoulsA', 'OffsideA', 'CornersA', 'ChancesA',\n",
    "           'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = pd.DataFrame(data=None, columns=matches_columns)\n",
    "\n",
    "# Table for goal information, can be linked through \"Link\" with game info\n",
    "goals_columns = ['Season', 'Gameday', 'Link', 'Score', 'Minute', 'Player']\n",
    "goals_df = pd.DataFrame(data=None, columns=goals_columns)\n",
    "\n",
    "# Table for Starting 11\n",
    "lineup_columns = [\"Link\"] + [\"Home_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)] + [\"Away_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)]\n",
    "lineup_df = pd.DataFrame(data=None, columns=lineup_columns)\n",
    "\n",
    "for year in range(2014,2015):\n",
    "    for gameday in range(1,35):\n",
    "        # Write URL to access kicker gameday website\n",
    "        url = \"http://www.kicker.de/news/fussball/bundesliga/spieltag/1-bundesliga/\"\n",
    "        url = url + str(year) + \"-\" + str(year+1)[2:4] + \"/\"\n",
    "        url = url + str(gameday) + \"/0/spieltag.html\"\n",
    "        \n",
    "        file = ''\n",
    "        while file =='':\n",
    "            try:\n",
    "                file = requests.get(url)\n",
    "            except:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "        # Download and parse page\n",
    "        main = BeautifulSoup(file.content, 'html.parser')\n",
    "\n",
    "        main_table = main.find('table', {'class': 'tStat', 'summary': 'Tabelle'})\n",
    "        \n",
    "        main_matches = main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'})\n",
    "\n",
    "        # Find all links of class \"link.\" These refer to the game analyses.\n",
    "        for game in main_matches.find_all('a', {'class' : 'link'}):\n",
    "            game_url = game.get('href')\n",
    "            encoded_in_url = game_url.split(\"/\")\n",
    "\n",
    "            # Game ID is the article ID for kicker, as unique ID for game\n",
    "            game_id = encoded_in_url[8][0:7]\n",
    "\n",
    "            # Game URL encodes the teams which are playing\n",
    "            teams = encoded_in_url[9][13:-5].split(\"_\")\n",
    "            teams = [teams[i][teams[i].rfind('-')+1:] for i in range(2)]\n",
    "\n",
    "            game_url = \"http://www.kicker.de\" + game_url\n",
    "            hist_file = requests.get(game_url.replace(\"spielanalyse\", \"direktvergleichliga\"))\n",
    "            hist_soup = BeautifulSoup(hist_file.content, 'html.parser')\n",
    "\n",
    "            # Can't use \"Direktvergleich\" because it includes all matches up to date of download and not to date of that game.\n",
    "            # hist = hist_soup.find('table', {'class': 'tStat', 'summary': 'Direktvergleich'})\n",
    "            # record = map(int, map(str.strip, str(hist.find('td', {'class': \"alignright spielinfo_wert\"}).text).split('/')))\n",
    "            hist = hist_soup.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'})\n",
    "            results = []\n",
    "            try:\n",
    "                for g in hist.find_all('td', {'class': 'alignleft nowrap'}):\n",
    "                    link = g.next_sibling.next_sibling.find('a').get('href')\n",
    "                    encoded_in_url = link.split(\"/\")\n",
    "\n",
    "                    res_year = int(encoded_in_url[6][0:4])\n",
    "                    res_gday = int(encoded_in_url[7])\n",
    "\n",
    "                    if res_year < int(year) or (res_year == year and res_gday < gameday):\n",
    "                        result = g.text\n",
    "                        result = map(int, result[0:result.find('(')].strip().split(\":\"))\n",
    "                        result = result[0]-result[1]\n",
    "                        # If the historical game was Team B - Team A instead of Team A - Team B today, then flip result\n",
    "                        if link.find(teams[0] + \".html\") != -1:\n",
    "                            result = -result\n",
    "                        results.append(result)\n",
    "                    if len(results) >= 6:\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            results += [np.nan] * (6 - len(results))\n",
    "\n",
    "\n",
    "            # Open website with statistics from the game and parse.\n",
    "            game_file = requests.get(game_url)\n",
    "            game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "            \n",
    "            error_count = 0\n",
    "            while game_soup.find('table', {'class': 'tStat', 'summary': 'Vereinsliste'}) == None:\n",
    "                game_file = requests.get(game_url)\n",
    "                game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "                error_count += 1\n",
    "                \n",
    "                if error_count == 3:\n",
    "                    print \"Error on game \" + str(game_id)\n",
    "                    break\n",
    "\n",
    "            # Download lineups\n",
    "            if error_count < 3:\n",
    "                both_teams = game_soup.find('table', {'class': 'tStat', 'summary': 'Vereinsliste'})\n",
    "                lineup = []\n",
    "                for one_team in both_teams.find_all('div', {'class': 'aufstellung_team'}):\n",
    "                    for player in one_team.find_all('a', {'class': 'link_noicon'}):\n",
    "                        lineup.append(player.get('href').split(\"/\")[8])\n",
    "                        # Make sure to only download the first 11, the Starting 11, for each team\n",
    "                        if np.mod(len(lineup), 11) == 0:\n",
    "                            break\n",
    "                lineup = [int(game_id)] + map(int, lineup)\n",
    "            else:\n",
    "                lineup = [int(game_id)] + [np.nan for _ in range(22)]\n",
    "                \n",
    "            lineup_df = lineup_df.append(pd.DataFrame(data=[lineup], columns=lineup_columns), ignore_index=True)\n",
    "\n",
    "            # Download main stats and save in a list.\n",
    "            stats = game_soup.find('table', {'class': 'tStat tStatKarten', 'summary': 'Berufungen'})\n",
    "            team1_stats = []\n",
    "            team2_stats = []\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignleft first\"}):\n",
    "                    if stat.text != ('angekommene P채sse').decode('utf-8') and stat.text != ('Ecken').decode('utf-8') and stat.text != ('Fehlp채sse').decode('utf-8') and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                        team1_stats.append(stat.next_sibling.next_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team1_stats = [np.NaN for _ in range(8)]\n",
    "                team1_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainH'}).text)\n",
    "            try:\n",
    "                for stat in stats.find_all('td', {'class': \"alignright last\"}):\n",
    "                    if stat.text != ('angekommene P채sse').decode('utf-8') and stat.text != ('Ecken').decode('utf-8') and stat.text != ('Fehlp채sse').decode('utf-8') and stat.text != ('Gefoult worden').decode('utf-8'):\n",
    "                        team2_stats.append(stat.previous_sibling.previous_sibling.text.strip(\"\\%\"))\n",
    "            except:\n",
    "                team2_stats = [np.NaN for _ in range(8)]\n",
    "                team2_stats[0] = int(game_soup.find('div', {'id': 'ovBoardExtMainA'}).text)\n",
    "\n",
    "            # For whatever reason, number of chances is stored in a different table. Grab this and add to list.     \n",
    "            for value in game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_kickerDaten', 'ctl00_PlaceHolderHalf_ctl04_kickerDaten']):\n",
    "                corners = value.find('div', {'class': 'ecken'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(corners[0]))\n",
    "                team2_stats.append(int(corners[1]))\n",
    "                chances = value.find('div', {'class': 'chancen'}).findNext('div').text.split(':')\n",
    "                team1_stats.append(int(chances[0]))\n",
    "                team2_stats.append(int(chances[1]))\n",
    "\n",
    "            # Download number of people in attendance\n",
    "            for value in game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_zuschauer', 'ctl00_PlaceHolderHalf_ctl04_zuschauer']):\n",
    "                attendance = value.find('div', {'class': 'wert'}).text\n",
    "                if attendance.find('(') > 0:\n",
    "                    # \"Error\" handling if stadion is sold out\n",
    "                    attendance = attendance[0:attendance.find('(')]\n",
    "                attendance = int(attendance.strip())\n",
    "\n",
    "\n",
    "            # Write stats for game into dataframe\n",
    "\n",
    "            feed = [year] + [gameday] + [game_id] + teams + team1_stats + team2_stats + [attendance] + results\n",
    "            feed += [np.nan] * (len(matches_columns) - len(feed))\n",
    "            matches_df = matches_df.append(pd.DataFrame(data=[feed], columns=matches_columns), ignore_index=True)\n",
    "\n",
    "            # Find the scorers in the game and download the info\n",
    "            goal_list = []\n",
    "            goals_cards = game_soup.find('table', {'class': 'tStat', 'summary': 'Tore & Karten'})\n",
    "            goals = goals_cards.find('div', {'class': 'tore_karten'}).find_all('div', {'class': 'kompletteZeile'})\n",
    "\n",
    "            for goal in goals:\n",
    "                goal_text = goal.text\n",
    "                goal_link = goal.find('a')\n",
    "                player = goal_link.get('href').split(\"/\")[8]\n",
    "\n",
    "                pos = goal_text.find(\":\")\n",
    "                score = int(goal_text[pos-1:pos])-int(goal_text[pos+1:pos+2])\n",
    "                pos = goal_text.find(\".,\")\n",
    "                # \"Error\" handling for when a goal is scored in overtime. Treat as 45' and 90'.\n",
    "                if pos < 0:\n",
    "                    pos = goal_text.find(\". +\") \n",
    "                minute = goal_text[0:pos]\n",
    "                pos = minute.find(\"(\")\n",
    "                minute = int(minute[pos+1:])\n",
    "\n",
    "                goal_list.append(map(int,[year, gameday, game_id, score, minute, player]))\n",
    "\n",
    "            # If goals fell, write info into goal dataframe.    \n",
    "            if len(goal_list) > 0:\n",
    "                goals_df = goals_df.append(pd.DataFrame(data=goal_list, columns=goals_columns), ignore_index=True)\n",
    "                \n",
    "for col in matches_df.columns:\n",
    "    try:\n",
    "        matches_df.loc[:, col] = matches_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "matches_df.loc[:, \"GD\"] = matches_df.loc[:, \"GoalsH\"] - matches_df.loc[:, \"GoalsA\"]\n",
    "\n",
    "matches_columns_ro = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA', 'GD',\n",
    "                      'GoalsH', 'GoalsA', 'ShotsH', 'ShotsA', 'PassesH', 'PassesA', 'PassQuH', 'PassQuA', 'PossesH', 'PossesA', \n",
    "                      'ChallH', 'ChallA', 'FoulsH', 'FoulsA', 'OffsideH', 'OffsideA', 'CornersH', 'CornersA', 'ChancesH', 'ChancesA',\n",
    "                      'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = matches_df.loc[:, matches_columns_ro]\n",
    "\n",
    "for col in goals_df.columns:\n",
    "    goals_df.loc[:, col] = goals_df.loc[:, col].astype(int)\n",
    "\n",
    "for col in lineup_df.columns:\n",
    "    lineup_df.loc[:, col] = lineup_df.loc[:, col].astype(int)\n",
    "\n",
    "    \n",
    "matches_df.to_csv(\"Matches_2014.csv\")\n",
    "goals_df.to_csv(\"Goals_2014.csv\")\n",
    "lineup_df.to_csv(\"Lineup_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://www.kicker.de/news/fussball/bundesliga/spieltag/1-bundesliga/2016-17/spieltag.html\"\n",
    "main = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "main_standings = main.find('table', {'class': 'tStat', 'summary': 'Tabelle'}).find_all('a', {'class': 'link verinsLinkBild'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15778, 9, 3, 0, 27, 10]\n",
      "[14, 8, 3, 1, 26, 8]\n",
      "[29, 7, 3, 2, 19, 11]\n",
      "[32, 7, 3, 2, 18, 10]\n",
      "[16, 6, 4, 2, 18, 8]\n",
      "[3209, 5, 7, 0, 21, 14]\n",
      "[17, 6, 3, 3, 27, 14]\n",
      "[2, 5, 2, 5, 17, 13]\n",
      "[30, 5, 2, 5, 22, 22]\n",
      "[9, 5, 1, 6, 19, 20]\n",
      "[7, 5, 0, 7, 16, 24]\n",
      "[91, 3, 4, 5, 10, 14]\n",
      "[15, 3, 4, 5, 12, 18]\n",
      "[24, 2, 4, 6, 10, 15]\n",
      "[98, 2, 2, 8, 11, 24]\n",
      "[4, 2, 2, 8, 15, 31]\n",
      "[7659, 1, 3, 8, 9, 22]\n",
      "[12, 0, 4, 8, 8, 27]\n"
     ]
    }
   ],
   "source": [
    "for team in main_standings:\n",
    "            # Start with the link for the team as our anchor\n",
    "            current_tag = team\n",
    "            # Split up the URL that encodes the team name & ID, and then separate the team ID from the teamname\n",
    "            team_id = int(current_tag.get('href').split('/')[7].split('-')[-1])\n",
    "            # Move to the win-draw-loss record\n",
    "            current_tag = current_tag.parent.parent.parent\n",
    "            current_tag = moveSibling(current_tag, 8)\n",
    "            rec_w = int(current_tag.text)\n",
    "            current_tag = moveSibling(current_tag, 2)\n",
    "            rec_d = int(current_tag.text)\n",
    "            current_tag = moveSibling(current_tag, 2)\n",
    "            rec_l = int(current_tag.text)\n",
    "            current_tag = moveSibling(current_tag, 4)\n",
    "            goals = map(int, current_tag.text.split(\":\"))\n",
    "            \n",
    "            print [team_id] + [rec_w] + [rec_d] + [rec_l] + goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set up dataframes to store the downloaded data\n",
    "# Table for game information\n",
    "matches_columns = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA',\n",
    "           'GoalsH', 'ShotsH', 'PassesH', 'PassQuH', 'PossesH', 'ChallH', 'FoulsH', 'OffsideH', 'CornersH', 'ChancesH',\n",
    "           'GoalsA', 'ShotsA', 'PassesA', 'PassQuA', 'PossesA', 'ChallA', 'FoulsA', 'OffsideA', 'CornersA', 'ChancesA',\n",
    "           'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = pd.DataFrame(data=None, columns=matches_columns)\n",
    "\n",
    "# Table for goal information, can be linked through \"Link\" with game info\n",
    "goals_columns = ['Season', 'Gameday', 'Link', 'Score', 'Minute', 'Player']\n",
    "goals_df = pd.DataFrame(data=None, columns=goals_columns)\n",
    "\n",
    "# Table for Starting 11\n",
    "lineup_columns = [\"Link\"]\n",
    "lineup_columns += [\"Home_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)] \n",
    "lineup_columns += [\"Away_Player\" + (\"0\" + str(i))[-2:] for i in range(1,12)]\n",
    "lineup_df = pd.DataFrame(data=None, columns=lineup_columns)\n",
    "\n",
    "error_count = 0 # Counts failed attempts at opening / reading before abort.\n",
    "\n",
    "for year in range(2004,2005):\n",
    "    for gameday in range(1,2):\n",
    "        # Write URL to access kicker gameday website\n",
    "        url = \"http://www.kicker.de/news/fussball/bundesliga/spieltag/1-bundesliga/\"\n",
    "        url = url + str(year) + \"-\" + str(year+1)[2:4] + \"/\"\n",
    "        url = url + str(gameday) + \"/0/spieltag.html\"\n",
    "\n",
    "        # Try 3 times to open the website and search the table with the matches played on this gameday\n",
    "        error_count = 0\n",
    "        while error_count < 3:\n",
    "            try:\n",
    "                website = requests.get(url)\n",
    "                main = BeautifulSoup(website.content, 'html.parser') \n",
    "                main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'}).find_all('a', {'class': 'link verinsLinkBild'})\n",
    "                main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Tabelle'}).find_all('a', {'class' : 'link'})\n",
    "                break\n",
    "            except:\n",
    "                error_count += 1\n",
    "                print error_count\n",
    "        # If opening fails, skip gameday.\n",
    "        if error_count == 3:\n",
    "            continue\n",
    "        \n",
    "        # Analyze standings as they are AFTER the games that have been played. Post-Analysis follows.\n",
    "        main_standings = main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'x'})\n",
    "        standings = []\n",
    "        rank = 1\n",
    "        for team in main_standings:\n",
    "            # Start with the link for the team as our anchor\n",
    "            current_tag = team.get('href')\n",
    "            # Split up the URL that encodes the team name & ID, and then separate the team ID from the teamname\n",
    "            team_id = int(current_tag.split('/')[7].split('-')[-1])\n",
    "            # Move to the win-draw-loss record\n",
    "            current_tag = current_tag.parent.parent.parent\n",
    "            current_tag = moveSibling(current_tag, 8)\n",
    "            rec_w = int(current_tag.text)\n",
    "            current_tag = moveSibling(current_tag, 2)\n",
    "            rec_d = int(current_tag.text)\n",
    "            current_tag = moveSibling(current_tag, 2)\n",
    "            rec_l = int(current_tag.text)\n",
    "            # Move to goals scored and conceeded\n",
    "            current_tag = moveSibling(current_tag, 2)\n",
    "            goals = map(int, current_tag.text.split(\":\"))\n",
    "            \n",
    "            standings.append([gameday] + [rank] + [team_id] + [rec_w] + [rec_d] + [rec_l] + goals)\n",
    "            # Increase rank by one for the next team, regardless if there is a tie\n",
    "            rank += 1\n",
    "            \n",
    "        \n",
    "        main_matches = main.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'})\n",
    "        \n",
    "        for game in main_matches.find_all('a', {'class' : 'link'}):\n",
    "            game_url = game.get('href')\n",
    "            encoded_in_url = game_url.split(\"/\")\n",
    "            \n",
    "            # Game ID is the article ID for kicker, as unique ID for game\n",
    "            game_id = encoded_in_url[8][0:7]\n",
    "\n",
    "            # Game URL encodes the teams which are playing\n",
    "            teams = encoded_in_url[9][13:-5].split(\"_\")\n",
    "            teams = [teams[i][teams[i].rfind('-')+1:] for i in range(2)]\n",
    "\n",
    "            # URL to open individiual games\n",
    "            game_url = \"http://www.kicker.de\" + game_url\n",
    "            \n",
    "            # Open website with report from the game and parse.\n",
    "            # Open website with historical matchup data.\n",
    "            # Try 3 times to open the website and search the table with the matches played on this gameday\n",
    "            error_count = 0\n",
    "            while error_count < 3:\n",
    "                try:\n",
    "                    game_file = requests.get(game_url)\n",
    "                    game_soup = BeautifulSoup(game_file.content, 'html.parser')\n",
    "                    # Check if lineup can be found\n",
    "                    game_soup.find('table', {'class': 'tStat', 'summary': 'Vereinsliste'}).find_all('div', {'class': 'aufstellung_team'})\n",
    "                    # Check if corners, chances and attendance can be found\n",
    "                    game_soup.findAll('div', id=['ctl00_PlaceHolderHalf_ctl03_kickerDaten', 'ctl00_PlaceHolderHalf_ctl04_kickerDaten'])[0].find('div', {'class': 'ecken'})\n",
    "                    # Check if goal list can be found\n",
    "                    #game_soup.find('table', {'class': 'tStat', 'summary': 'Tore & Karten'}).find('div', {'class': 'tore_karten'})\n",
    "                    # Skip check for stats since this can only be found in data past 2013\n",
    "                except:\n",
    "                    error_count += 1\n",
    "                    print \"error\"\n",
    "            # If opening fails, skip match.\n",
    "            if error_count == 3:\n",
    "                continue\n",
    "            # Same procedure for historical data        \n",
    "            error_count = 0\n",
    "            while error_count < 3:\n",
    "                try:\n",
    "                    hist_file = requests.get(game_url.replace(\"spielanalyse\", \"dirgektvergleichliga\"))\n",
    "                    hist_soup = BeautifulSoup(hist_file.content, 'html.parser')\n",
    "                    # Check if historical matches can be found\n",
    "                    hist_soup.find('table', {'class': 'tStat tab1-bundesliga', 'summary': 'Begegnungen'}).find_all('td', {'class': 'alignleft nowrap'})\n",
    "                    break\n",
    "                except:\n",
    "                    error_count += 1\n",
    "            # If opening fails, skip match.\n",
    "            if error_count == 3:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "        \n",
    "        print \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in matches_df.columns:\n",
    "    try:\n",
    "        matches_df.loc[:, col] = matches_df.loc[:, col].astype(int)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "matches_df.loc[:, \"GD\"] = matches_df.loc[:, \"GoalsH\"] - matches_df.loc[:, \"GoalsA\"]\n",
    "\n",
    "matches_columns_ro = ['Season', 'Gameday', 'Link', 'TeamH', 'TeamA', 'GD',\n",
    "                      'GoalsH', 'GoalsA', 'ShotsH', 'ShotsA', 'PassesH', 'PassesA', 'PassQuH', 'PassQuA', 'PossesH', 'PossesA', \n",
    "                      'ChallH', 'ChallA', 'FoulsH', 'FoulsA', 'OffsideH', 'OffsideA', 'CornersH', 'CornersA', 'ChancesH', 'ChancesA',\n",
    "                      'Attendance', 'Last6-1', 'Last6-2', 'Last6-3', 'Last6-4', 'Last6-5', 'Last6-6']\n",
    "matches_df = matches_df.loc[:, matches_columns_ro]\n",
    "\n",
    "for col in goals_df.columns:\n",
    "    goals_df.loc[:, col] = goals_df.loc[:, col].astype(int)\n",
    "\n",
    "for col in lineup_df.columns:\n",
    "    lineup_df.loc[:, col] = lineup_df.loc[:, col].astype(int)\n",
    "\n",
    "    \n",
    "matches_df.to_csv(\"Matches_2014.csv\")\n",
    "goals_df.to_csv(\"Goals_2014.csv\")\n",
    "lineup_df.to_csv(\"Lineup_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_df = pd.read_csv('Matches_2004.csv', index_col=0).iloc[378:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_df.to_csv('Matches_2004.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_df.index  = matches_df.index -378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches_df[matches_df['Gameday'] == 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
